import os
import time
import sqlite3
from functools import lru_cache
from backend.services.schema_discovery import SchemaDiscovery
from backend.services.document_processor import DocumentProcessor

class QueryCache:
    def __init__(self, ttl=300):
        # ttl = time to live (cache valid for 300 seconds = 5 min)
        self.cache = {}
        self.ttl = ttl

    def get(self, key):
        if key in self.cache:
            result, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                return result
        return None

    def set(self, key, value):
        self.cache[key] = (value, time.time())


class QueryEngine:
    def __init__(self, connection_string: str):
        # Initialize schema discovery
        self.schema_discovery = SchemaDiscovery(connection_string)
        self.schema = self.schema_discovery.analyze_database()
        self.cache = QueryCache()

        # Database connection
        db_path = connection_string.replace("sqlite:///", "")
        self.conn = sqlite3.connect(db_path)
        
        # Document processor
        self.doc_processor = DocumentProcessor()
        self.doc_processor.process_documents([  
            "backend/docs/resume1.txt",
            "backend/docs/resume2.txt"
        ])


    # Step 1: Query Classification
    def classify_query(self, query: str) -> str:
        """
        Classify the query type: SQL / Document / Hybrid
        """
        q = query.lower()

        # 1. Hybrid queries → check first
        if "compare" in q or ("database" in q and "document" in q):
            return "hybrid"

        # 2. Document-only
        doc_keywords = ["resume", "review", "contract", "document"]
        if any(word in q for word in doc_keywords):
            return "document"

        # 3. Default → SQL-like
        return "sql"

    
    # Step 2: Schema Mapping Stub
    def map_to_schema(self, query: str) -> str:
        """
        Simple keyword → schema table/column normalization
        (later we can use embeddings for fuzzy matching)
        """
        mapped_query = query.lower()
        mappings = {
            "employee": "employees",
            "staff": "employees",
            "salary": "salary",
            "pay": "salary",
            "department": "departments",
            "dept": "departments",
        }
        for word, mapped in mappings.items():
            mapped_query = mapped_query.replace(word, mapped)
        return mapped_query

    
    # Step 3: SQL Generation Rules
    def generate_sql(self, query: str) -> str | None:
        """
        Simple pattern-matching SQL generator (expand as needed).
        """
        q = query.lower()

        if "how many" in q and "employees" in q:
            return "SELECT COUNT(*) FROM employees;"

        if "average salary" in q:
            return """
            SELECT d.dept_name, AVG(e.salary)
            FROM employees e
            JOIN departments d ON e.dept_id = d.dept_id
            GROUP BY d.dept_name;
            """

        if "list employees" in q:
            return "SELECT emp_id, full_name, salary FROM employees;"

        if "highest salary" in q:
            return """
            SELECT full_name, salary
            FROM employees
            ORDER BY salary DESC
            LIMIT 1;
            """

        return None  # unsupported query

    
    # Step 4: SQL Executor + Cache
    @lru_cache(maxsize=50)
    def execute_sql(self, sql: str, limit: int = 20, offset: int = 0):
        # First, check cache
        cache_key = f"{sql}_{limit}_{offset}"
        cached = self.cache.get(cache_key)
        if cached:
            logging.info("Using cached result for a repeated query")
            return cached

        try:
            cursor = self.conn.cursor()

            # Add LIMIT + OFFSET (avoid adding twice!)
            paginated_sql = f"{sql.strip().rstrip(';')} LIMIT {limit} OFFSET {offset};"
            cursor.execute(paginated_sql)

            rows = cursor.fetchall()
            self.cache.set(cache_key, rows)
            return rows
        except Exception as e:
            return f"❌ SQL Execution Error: {e}"

    
    # Step 5: Full Query Pipeline
    def process_query(self, query: str):
        q_type = self.classify_query(query)

        if q_type == "sql":
            mapped_query = self.map_to_schema(query)
            sql = self.generate_sql(mapped_query)

            if not sql:
                return {
                    "status": "error",
                    "query_type": q_type,
                    "message": "Unsupported SQL query format",
                    "sql": None,
                }

            results = self.execute_sql(sql)
            return {
                "status": "ok" if not str(results).startswith("❌") else "error",
                "query_type": q_type,
                "sql": sql,
                "results": results,
            }

        elif q_type == "document":
            results = self.doc_processor.search(query)
            return {
                "status": "ok",
                "query_type": q_type,
                "results": results
            }

        elif q_type == "hybrid":
            # Hybrid handling
            # Step 1: run SQL (maybe fallback to generic employee list)
            sql = "SELECT emp_id, full_name, salary FROM employees;"
            db_results = self.execute_sql(sql)

            # Step 2: run document search
            doc_results = self.doc_processor.search(query)

            # Step 3: merge them
            return {
                "status": "ok",
                "query_type": q_type,
                "sql_used": sql,
                "db_results": db_results,
                "doc_results": doc_results
            }